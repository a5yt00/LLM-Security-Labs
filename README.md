<p align="center">
  <img src="https://img.shields.io/badge/OWASP-LLM%20Top%2010-red?style=for-the-badge&logo=owasp" />
  <img src="https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/Streamlit-Frontend-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" />
  <img src="https://img.shields.io/badge/Google%20Colab-GPU%20Backend-F9AB00?style=for-the-badge&logo=google-colab" />
</p>

<h1 align="center">ğŸ›¡ï¸ LLM Security Labs</h1>

<p align="center">
  <b>Interactive Red Teaming Environment for Large Language Models</b><br/>
  <i>7 Hands-on Labs Â· OWASP Top 10 for LLMs 2025 Â· Attack & Defense</i>
</p>

---

## ğŸ¯ What is this?

**LLM Security Labs** is a purpose-built training environment for learning how to attack â€” and defend â€” Large Language Models. It features a **GPU-powered backend** running on Google Colab and a sleek **Streamlit frontend** for interactive red teaming.

> âš ï¸ **Educational Use Only** â€” Practice responsible disclosure. Test only on systems you own.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ–¥ï¸  Local Machine      â”‚         â”‚   â˜ï¸  Google Colab (GPU)  â”‚
â”‚                          â”‚  HTTPS  â”‚                          â”‚
â”‚   Streamlit Frontend     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   FastAPI Backend         â”‚
â”‚   (lab_app.py)           â”‚  tunnel â”‚   Ollama + Phi-3 LLM     â”‚
â”‚                          â”‚         â”‚   ChromaDB Vector Store   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª Labs

| # | Lab | OWASP Category | Description |
|---|-----|---------------|-------------|
| 1 | **ğŸ“„ Poisoned RAG** | LLM04 â€” Data Poisoning | Hidden white text in PDFs hijacks RAG retrieval |
| 2 | **ğŸ¤– Agent Exploitation** | LLM06 â€” Excessive Agency | Trick AI agents into executing dangerous tools |
| 3 | **ğŸ’£ Filter Bypass** | LLM01 â€” Prompt Injection | Evade content filters via emoji, unicode, ROT13, base64 |
| 4 | **ğŸ”“ Prompt Extraction** | LLM02 â€” Sensitive Disclosure | Extract confidential system prompts from LLMs |
| 5 | **ğŸ‘» Invisible Unicode** | LLM01 â€” Prompt Injection | Unicode Tag Block (U+E0000â€“E007F) invisible injections |
| 6 | **ğŸ“¤ Context Leakage** | LLM02 â€” Sensitive Disclosure | Extract the LLM's entire context window |
| 7 | **ğŸ”¢ Tokenizer Visualizer** | Utility | Visualize how LLMs tokenize your input |

---

## ğŸš€ Setup Guide

### Prerequisites

- **Python 3.10+** installed locally
- A **Google account** for Colab access
- Modern web browser

### Step 1 â€” Launch the Backend (Google Colab)

1. Open [`backend_lab.ipynb`](backend_lab.ipynb) in [Google Colab](https://colab.research.google.com/)
2. **Enable GPU**: `Runtime` â†’ `Change runtime type` â†’ **T4 GPU**
3. **Run all cells** in order:
   - Cell 1 â€” Installs dependencies (Ollama, FastAPI, ChromaDB, etc.)
   - Cell 2 â€” Starts Ollama and pulls the `phi3` model onto the GPU
   - Cell 3 â€” Creates the vulnerable FastAPI server (`server.py`)
   - Cell 4 â€” Starts the server and generates a **public Cloudflare tunnel URL**
4. ğŸ“‹ **Copy the URL** that looks like: `https://xxxx-xxxx.trycloudflare.com`

### Step 2 â€” Launch the Frontend (Local)

```bash
# Clone the repository
git clone https://github.com/a5yt00/LLM-Security-Labs.git
cd LLM-Security-Labs

# Install dependencies
pip install -r requirements.txt

# Run the Streamlit app
streamlit run lab_app.py
```

### Step 3 â€” Connect

1. The Streamlit app opens at `http://localhost:8501`
2. Paste your **Colab tunnel URL** into the sidebar
3. Start hacking! ğŸ‰

---

## ğŸ“ Project Structure

```
LLM-Security-Labs/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                  # You are here
â”œâ”€â”€ ğŸ lab_app.py                 # Streamlit frontend (7 labs)
â”œâ”€â”€ ğŸ““ backend_lab.ipynb          # Colab GPU backend notebook
â”œâ”€â”€ ğŸ“¦ requirements.txt           # Python dependencies
â”‚
â””â”€â”€ ğŸ“‚ LLM_Security_Labs/
    â”œâ”€â”€ ğŸ“– LAB_MANUAL.md          # Detailed lab guide & attack reference
    â”œâ”€â”€ ğŸ““ backend_lab.ipynb      # Backend notebook (copy)
    â”œâ”€â”€ ğŸ lab_app.py             # Frontend (copy)
    â””â”€â”€ ğŸ“¦ requirements.txt       # Dependencies
```

---

## ğŸ›¡ï¸ Defenses Explored

Each lab demonstrates both the **attack** and the **defense**:

| Defense | How it Works |
|---------|-------------|
| ğŸ§‘â€âš–ï¸ **Human-in-the-Loop** | Require approval for sensitive tool calls |
| ğŸ”’ **Tool Whitelisting** | Restrict which tools the agent can invoke |
| ğŸª– **Prompt Hardening** | System-level instructions to resist extraction |
| ğŸ” **Output Filtering** | Detect prompt-like patterns in responses |
| ğŸ§  **Vector Filtering** | Block semantically similar attack embeddings |

---

## ğŸ“š References

- [OWASP Top 10 for LLMs 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Keysight â€” Invisible Prompt Injection Research (2025)](https://www.keysight.com)
- [Ollama](https://ollama.com/) Â· [Streamlit](https://streamlit.io/) Â· [FastAPI](https://fastapi.tiangolo.com/)

---

<p align="center">
  <b>Built for learning. Break things responsibly. ğŸ”</b>
</p>
